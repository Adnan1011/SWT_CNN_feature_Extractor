name: "HC_Feats"

input: "raw_features"
input_dim: 1
input_dim: 1
input_dim: 224
input_dim: 224

# ------------------------ Shallow net over raw features -----------------------------

layer {
	name: "conv1"
	type: "Convolution"
	bottom: "raw_features"
	top: "conv1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 128
		kernel_size: 7
		pad: 3
		stride: 4
		weight_filler {
			type: "xavier"		
			std: 0.01		
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}

layer {
    name: "bnorm1"
	type: "BatchNorm"
	bottom: "conv1"
	top: "conv1"
}

layer {
    name: "scale1"
	type: "Scale"
	bottom: "conv1"
	top: "conv1"
	scale_param {
		filler {
			type: "xavier"
		}
		bias_term: true
	}
}

layer {
	name: "relu1"
	type: "ReLU"
	bottom: "conv1"
	top: "conv1"
}

layer {
	name: "conv2"
	type: "Convolution"
	bottom: "conv1"
	top: "conv2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 256
		kernel_size: 5
		pad: 2
		stride: 2
		weight_filler {
			type: "xavier"
			std: 0.01	
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}

layer {
    name: "bnorm2"
	type: "BatchNorm"
	bottom: "conv2"
	top: "conv2"
}

layer {
    name: "scale2"
	type: "Scale"
	bottom: "conv2"
	top: "conv2"
	scale_param {
		filler {
			type: "xavier"
		}
		bias_term: true
	}
}

layer {
	name: "relu2"
	type: "ReLU"
	bottom: "conv2"
	top: "conv2"
}

layer {
	name: "pool1"
	type: "Pooling"
	bottom: "conv2"
	top: "pool1"
	pooling_param {
		kernel_size: 3
		stride: 2
		pad: 1
		pool: MAX
	}
}


#----------------------- RPN -------------------------
layer {
   name: "conv_proposal1"
   type: "Convolution"
   bottom: "pool1"
   top: "conv_proposal1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 256
	   kernel_size: 3
	   pad: 1
	   stride: 1
	   weight_filler {
		 type: "xavier"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
    name: "bnorm3"
	type: "BatchNorm"
	bottom: "conv_proposal1"
	top: "conv_proposal1"
}

layer {
    name: "scale3"
	type: "Scale"
	bottom: "conv_proposal1"
	top: "conv_proposal1"
	scale_param {
		filler {
			type: "xavier"
		}
		bias_term: true
	}
}

layer {
   name: "relu_proposal1"
   type: "ReLU"
   bottom: "conv_proposal1"
   top: "conv_proposal1"
}

layer {
   name: "proposal_cls_score"
   type: "Convolution"
   bottom: "conv_proposal1"
   top: "proposal_cls_score"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 18   # 2(bg/fg) * 9(anchors) 
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "xavier"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "proposal_bbox_pred"
   type: "Convolution"
   bottom: "conv_proposal1"
   top: "proposal_bbox_pred"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 36	# 4 * 9(anchors) 
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "xavier"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

#-----------------------output------------------------

# to enable the calculation of softmax loss, we first reshape blobs related to SoftmaxWithLoss
layer {
   bottom: "proposal_cls_score"
   top: "proposal_cls_score_reshape"
   name: "proposal_cls_score_reshape"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 2
			dim: -1 
			dim: 0
		}
	}
}

layer {
   name: "proposal_cls_prob"
   type: "Softmax"
   bottom: "proposal_cls_score_reshape"
   top: "proposal_cls_prob"
}
